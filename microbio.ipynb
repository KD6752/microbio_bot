{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader,DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1: Load raw PDF(s)\n",
    "DATA_PATH=\"data/\"\n",
    "def load_pdf_files(data):\n",
    "    loader=DirectoryLoader(\n",
    "        data,\n",
    "        glob=\"*.pdf\",\n",
    "        loader_cls =PyPDFLoader)\n",
    "\n",
    "    documents=loader.load()\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of PDF pages: 969\n"
     ]
    }
   ],
   "source": [
    "documents = load_pdf_files(data=DATA_PATH)\n",
    "print(\"length of PDF pages:\",len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2:Create Chunks\n",
    "def create_chunks(extracted_data):\n",
    "    text_splitter= RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50)\n",
    "    text_chunks=text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of Text Chunks: 9742\n"
     ]
    }
   ],
   "source": [
    "text_chunks =create_chunks(extracted_data=documents)\n",
    "print(\"length of Text Chunks:\",len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create Vector Embeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "def get_embedding_model():\n",
    "    embedding_model=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kokildhakal/Desktop/STUDY/Generative AI/Langchain-Projects/microbio_bot/microbio_bot/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embedding_model=get_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 :store embedding in FAISS\n",
    "from langchain_community.vectorstores import FAISS\n",
    "DB_FAISS_PATH=\"vectorstore/db_faiss\"\n",
    "db=FAISS.from_documents(text_chunks,embedding_model)\n",
    "db.save_local(DB_FAISS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: Setup LLM (Mistral with HuggingFace)\n",
    "HUGGINGFACE_API_KEY= os.environ.get(\"HUGGINGFACE_API_KEY\")\n",
    "HUGGINGFACE_REPO_ID=\"mistralai/Mistral-7B-Instruct-v0.3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm(huggingface_repo_id):\n",
    "    llm=HuggingFaceEndpoint(\n",
    "        repo_id = huggingface_repo_id,\n",
    "        temperature =0.5,\n",
    "        model_kwargs={\n",
    "            \"token\":HUGGINGFACE_API_KEY,\n",
    "            \"max_length\": \"512\"\n",
    "                    })\n",
    "    \n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Connect LLm with FAISS and Create chain\n",
    "\n",
    "DB_FAISS_PATH=\"vectorstore/db_faiss\"\n",
    "\n",
    "CUSTOM_PROMPT_TEMPLATE= \"\"\"\n",
    "use the pieces of information provided in the context to answer uwer's question.\n",
    "if you don't know the answer, just say that you dont know, don't try to make up an answer.\n",
    "Don't provide anythin out of the given context\n",
    "context:{context}\n",
    "Question:{question}\n",
    "Start the answer directly. No small talk please.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_custom_prompt(custom_prompt_template):\n",
    "    prompt= PromptTemplate(template=custom_prompt_template,input_variable=[\"context\",\"question\"])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load database\n",
    "embedding_model=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "db = FAISS.load_local(DB_FAISS_PATH,embedding_model,allow_dangerous_deserialization =True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create QA chain\n",
    "HUGGINGFACE_REPO_ID=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "qa_chain=RetrievalQA.from_chain_type(\n",
    "    llm=load_llm(HUGGINGFACE_REPO_ID),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={'k':3}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\":set_custom_prompt(CUSTOM_PROMPT_TEMPLATE)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kokildhakal/Desktop/STUDY/Generative AI/Langchain-Projects/microbio_bot/microbio_bot/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT: Coliform bacteria can be differentiated from other Gram-negative bacteria based on their ability to ferment lactose and produce gas. This is tested using a medium called MacConkey agar, which selects for lactose-fermenting bacteria and inhibits the growth of other bacteria. If a colony on MacConkey agar appears red, it indicates the presence of lactose-fermenting bacteria, which are likely coliform bacteria. However, it's important to note that not all coliform bacteria produce gas, so this test should be used in combination with other tests for accurate identification.\n"
     ]
    }
   ],
   "source": [
    "# Invoke with a single query\n",
    "user_query= \"how to differentiate coliform from other Gram negative bacteria?\"\n",
    "response =qa_chain.invoke({'query':user_query})\n",
    "print(\"RESULT:\",response[\"result\"])\n",
    "# print(\"SOURCE DOCUMENTS:\",response[\"source_documents\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
